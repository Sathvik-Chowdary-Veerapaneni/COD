{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model and tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "# model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/nli-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(sentence, tokenizer, model):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state  # Shape: (1, sequence_length, hidden_size)\n",
    "    attention_mask = inputs['attention_mask']  # Shape: (1, sequence_length)\n",
    "    mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()  # Shape: (1, sequence_length, hidden_size)\n",
    "    sum_embeddings = torch.sum(embeddings * mask_expanded, dim=1)  # Shape: (1, hidden_size)\n",
    "    sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)  # Shape: (1, hidden_size)\n",
    "    mean_pooled_embeddings = sum_embeddings / sum_mask  # Shape: (1, hidden_size)\n",
    "    return mean_pooled_embeddings.squeeze(0)  # Shape: (hidden_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined list of sentences\n",
    "sentences = [\n",
    "    \"This sentence has a similar theme.\",\n",
    "    \"An entirely different topic is discussed here.\",\n",
    "    \"Here's another sentence that's somewhat related.\",\n",
    "    \"This is the base sentence for comparison.\",\n",
    "    \"Random words football tree monitor.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of 25 passages from different fields\n",
    "passages = [\n",
    "    'For example, I would like to enhance a new website with no code tools, such that, users not can spend more time on thinking compared to more on writing code',\n",
    "    'A new medical field, to discover monkey DNA, with human research, can lead a new generation, to do this i can verify wih high end hosipitals']\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute embeddings and store in a dictionary\n",
    "sentence_embeddings = {}\n",
    "for sentence in sentences:\n",
    "    sentence_embeddings[sentence] = get_sentence_embedding(sentence, tokenizer, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find most similar sentence\n",
    "def find_most_similar(new_sentence, sentence_embeddings):\n",
    "    new_embedding = get_sentence_embedding(new_sentence, tokenizer, model)  # Shape: (hidden_size,)\n",
    "    highest_similarity = -1\n",
    "    most_similar_sentence = \"\"\n",
    "    \n",
    "    for sentence, embedding in sentence_embeddings.items():\n",
    "        # Both embeddings are 1D tensors of shape (hidden_size,)\n",
    "        similarity = torch.nn.functional.cosine_similarity(new_embedding, embedding, dim=0).item()\n",
    "        if similarity > highest_similarity:\n",
    "            highest_similarity = similarity\n",
    "            most_similar_sentence = sentence\n",
    "            \n",
    "    return most_similar_sentence, highest_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most similar sentence to 'Check this sentence for similarity.' is 'This is the base sentence for comparison.' with a similarity of 0.68\n"
     ]
    }
   ],
   "source": [
    "# Example new sentence\n",
    "new_sentence = \"Check this sentence for similarity.\"\n",
    "most_similar, similarity_score = find_most_similar(new_sentence, sentence_embeddings)\n",
    "print(f\"The most similar sentence to '{new_sentence}' is '{most_similar}' with a similarity of {similarity_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "\n",
    "# # Load model and tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "# model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "\n",
    "# def get_sentence_embedding(sentence, tokenizer, model):\n",
    "#     inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     embeddings = outputs.last_hidden_state  # Shape: (1, sequence_length, hidden_size)\n",
    "#     attention_mask = inputs['attention_mask']  # Shape: (1, sequence_length)\n",
    "#     mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()  # Shape: (1, sequence_length, hidden_size)\n",
    "#     sum_embeddings = torch.sum(embeddings * mask_expanded, dim=1)  # Shape: (1, hidden_size)\n",
    "#     sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)  # Shape: (1, hidden_size)\n",
    "#     mean_pooled_embeddings = sum_embeddings / sum_mask  # Shape: (1, hidden_size)\n",
    "#     return mean_pooled_embeddings.squeeze(0)  # Shape: (hidden_size,)\n",
    "\n",
    "# # Predefined list of sentences\n",
    "# sentences = [\n",
    "#     \"This sentence has a similar theme.\",\n",
    "#     \"An entirely different topic is discussed here.\",\n",
    "#     \"Here's another sentence that's somewhat related.\",\n",
    "#     \"This is the base sentence for comparison.\",\n",
    "#     \"Random words football tree monitor.\"\n",
    "# ]\n",
    "\n",
    "# # Precompute embeddings and store in a dictionary\n",
    "# sentence_embeddings = {}\n",
    "# for sentence in sentences:\n",
    "#     sentence_embeddings[sentence] = get_sentence_embedding(sentence, tokenizer, model)\n",
    "\n",
    "# # Function to find most similar sentence\n",
    "# def find_most_similar(new_sentence, sentence_embeddings):\n",
    "#     new_embedding = get_sentence_embedding(new_sentence, tokenizer, model)  # Shape: (hidden_size,)\n",
    "#     highest_similarity = -1\n",
    "#     most_similar_sentence = \"\"\n",
    "    \n",
    "#     for sentence, embedding in sentence_embeddings.items():\n",
    "#         # Both embeddings are 1D tensors of shape (hidden_size,)\n",
    "#         similarity = torch.nn.functional.cosine_similarity(new_embedding, embedding, dim=0).item()\n",
    "#         if similarity > highest_similarity:\n",
    "#             highest_similarity = similarity\n",
    "#             most_similar_sentence = sentence\n",
    "            \n",
    "#     return most_similar_sentence, highest_similarity\n",
    "\n",
    "# # Example new sentence\n",
    "# new_sentence = \"Check this sentence for similarity.\"\n",
    "# most_similar, similarity_score = find_most_similar(new_sentence, sentence_embeddings)\n",
    "# print(f\"The most similar sentence to '{new_sentence}' is '{most_similar}' with a similarity of {similarity_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute embeddings and store in a dictionary\n",
    "passage_embeddings = {}\n",
    "for passage in passages:\n",
    "    passage_embeddings[passage] = get_sentence_embedding(passage, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the top N most similar passages\n",
    "def find_top_n_similar(new_text, passage_embeddings, top_n=3):\n",
    "    new_embedding = get_sentence_embedding(new_text, tokenizer, model)  # Shape: (hidden_size,)\n",
    "    similarities = {}\n",
    "    for passage, embedding in passage_embeddings.items():\n",
    "        similarity = torch.nn.functional.cosine_similarity(new_embedding, embedding, dim=0).item()\n",
    "        similarities[passage] = similarity\n",
    "    # Sort the passages based on similarity scores in descending order\n",
    "    sorted_passages = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sorted_passages[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 passages similar to:\n",
      "'Flowers has great smell, where fragances of top brands puts billions of money, my idea is to extract a new smell and make billions which satisfies humans rich smell sense flavours'\n",
      "\n",
      "1. Similarity: 82.80%\n",
      "   Passage: A new medical field, to discover monkey DNA, with human research, can lead a new generation, to do this i can verify wih high end hosipitals\n",
      "\n",
      "2. Similarity: 79.84%\n",
      "   Passage: For example, I would like to enhance a new website with no code tools, such that, users not can spend more time on thinking compared to more on writing code\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example new text\n",
    "new_text = \"Flowers has great smell, where fragances of top brands puts billions of money, my idea is to extract a new smell and make billions which satisfies humans rich smell sense flavours\"\n",
    "\n",
    "# Find the top 3 most similar passages\n",
    "top_similar_passages = find_top_n_similar(new_text, passage_embeddings, top_n=3)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Top 3 passages similar to:\\n'{new_text}'\\n\")\n",
    "for i, (passage, similarity_score) in enumerate(top_similar_passages, start=1):\n",
    "    formatted_similarity = f\"{similarity_score * 100:.2f}%\"\n",
    "    print(f\"{i}. Similarity: {formatted_similarity}\\n   Passage: {passage}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = [\n",
    "    # Astronomy\n",
    "    \"The Milky Way galaxy contains hundreds of billions of stars, including our sun. It is a barred spiral galaxy with distinct arms, where new stars are constantly being formed from clouds of gas and dust. Astronomers believe that at the center of our galaxy lies a supermassive black hole, which exerts a strong gravitational pull on surrounding stars.\",\n",
    "    \n",
    "    # Medicine\n",
    "    \"Vaccination is a critical tool in public health, providing immunity against infectious diseases. Vaccines work by stimulating the immune system to recognize and fight pathogens like viruses and bacteria. Herd immunity can be achieved when a high percentage of the population is vaccinated, helping protect those who are unable to receive vaccines themselves.\",\n",
    "    \n",
    "    # Artificial Intelligence\n",
    "    \"Artificial Intelligence (AI) refers to the simulation of human intelligence in machines programmed to think and learn. Machine learning, a subset of AI, involves feeding algorithms vast amounts of data to identify patterns and make decisions. Applications of AI are vast, ranging from natural language processing to autonomous vehicles.\",\n",
    "    \n",
    "    # Zoology\n",
    "    \"Elephants are the largest land animals on Earth, known for their intelligence and strong social bonds. They communicate through a range of sounds, including low-frequency rumbles that can travel long distances. Elephants also exhibit behaviors such as mourning their dead, showcasing a level of emotional complexity rarely seen in other species.\",\n",
    "    \n",
    "    # Economics\n",
    "    \"Inflation is an economic phenomenon characterized by a general rise in prices over time. It can reduce the purchasing power of money, affecting both consumers and businesses. Central banks use monetary policy tools such as interest rate adjustments to manage inflation levels, aiming for a balance between economic growth and price stability.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Similarity: 90.40%\n",
      "   Passage: Elephants are the largest land animals on Earth, known for their intelligence and strong social bonds. They communicate through a range of sounds, including low-frequency rumbles that can travel long distances. Elephants also exhibit behaviors such as mourning their dead, showcasing a level of emotional complexity rarely seen in other species.\n",
      "\n",
      "2. Similarity: 87.90%\n",
      "   Passage: Artificial Intelligence (AI) refers to the simulation of human intelligence in machines programmed to think and learn. Machine learning, a subset of AI, involves feeding algorithms vast amounts of data to identify patterns and make decisions. Applications of AI are vast, ranging from natural language processing to autonomous vehicles.\n",
      "\n",
      "3. Similarity: 84.40%\n",
      "   Passage: Inflation is an economic phenomenon characterized by a general rise in prices over time. It can reduce the purchasing power of money, affecting both consumers and businesses. Central banks use monetary policy tools such as interest rate adjustments to manage inflation levels, aiming for a balance between economic growth and price stability.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load a more advanced pre-trained model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# \n",
    "# tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "# model = AutoModel.from_pretrained('sentence-transformers/nli-roberta-large')\n",
    "\n",
    "def get_sentence_embedding(sentence, tokenizer, model):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use mean pooling\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    masked_embeddings = embeddings * mask\n",
    "    summed = torch.sum(masked_embeddings, dim=1)\n",
    "    counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "    mean_pooled = summed / counts\n",
    "    # Normalize the embeddings\n",
    "    normalized_embeddings = torch.nn.functional.normalize(mean_pooled, p=2, dim=1)\n",
    "    return normalized_embeddings.squeeze(0)\n",
    "\n",
    "# Precompute embeddings\n",
    "passage_embeddings = {}\n",
    "for passage in passages:\n",
    "    passage_embeddings[passage] = get_sentence_embedding(passage, tokenizer, model)\n",
    "\n",
    "# Similarity function\n",
    "def find_top_n_similar(new_text, passage_embeddings, top_n=3, min_similarity_threshold=0.):\n",
    "    new_embedding = get_sentence_embedding(new_text, tokenizer, model)\n",
    "    similarities = {}\n",
    "    for passage, embedding in passage_embeddings.items():\n",
    "        similarity = torch.nn.functional.cosine_similarity(new_embedding.unsqueeze(0), embedding.unsqueeze(0), dim=1).item()\n",
    "        if similarity > min_similarity_threshold:\n",
    "            similarities[passage] = similarity\n",
    "    sorted_passages = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sorted_passages[:top_n]\n",
    "\n",
    "# Example new text\n",
    "new_text = ''' Renewable energy sources, such as solar and wind, are becoming increasingly important in the global effort to reduce carbon emissions and combat climate change\n",
    " Investments in green technology are expected to grow as countries aim for sustainable development and energy independence '''\n",
    "\n",
    "\n",
    "\n",
    "# Find top similar passages\n",
    "top_similar_passages = find_top_n_similar(new_text, passage_embeddings)\n",
    "\n",
    "# Display results\n",
    "for i, (passage, similarity_score) in enumerate(top_similar_passages, start=1):\n",
    "    formatted_similarity = f\"{similarity_score * 100:.2f}%\"\n",
    "    print(f\"{i}. Similarity: {formatted_similarity}\\n   Passage: {passage}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu)",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
